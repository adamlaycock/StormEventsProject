---
title: "report_draft"
author: "adamlaycock"
date: "2024-11-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-lib, warning=FALSE, message=FALSE}
library(tidyverse)
library(janitor)
library(workflows)
library(parsnip)
library(tidyclust)
library(tidymodels)
library(usmap)
library(gghighlight)
library(knitr)
library(xaringanExtra)
library(doParallel)
xaringanExtra::use_panelset()
```

```{r load-data}
details = read_rds('../../data/clean_data/details_clean.rds')
fatalities = read_rds('../../data/clean_data/fatalities_clean.rds')
```

# Damage Makeup by Event Type

```{r fig.align='center'}
cost_data <- details %>% 
  select(
    event_type, damage_property, damage_crops
  ) %>% 
    group_by(
      event_type
    ) %>% 
      summarise(
        Property = sum(
          damage_property, 
          na.rm=TRUE
        ),
        Crops = sum(
          damage_crops, 
          na.rm=TRUE
        )
      ) %>%
        slice_max(
          order_by = Property + Crops, 
          n = 20
        ) %>% 
          pivot_longer(
            cols=c('Property', 'Crops'),
            names_to='type',
            values_to='damage'
          )

cost_data %>% 
  ggplot(
    mapping=aes(
      x=damage/sum(damage)*100,
      y=reorder(event_type, damage),
      fill=type
    )
  ) +
  geom_col() +
  labs(
    title = 'Damage by Storm Event Type between 1996 & 2023', 
    subtitle = 'Total Damage (Property + Crops) was $227bn',
    y = 'Type of Event',
    x = 'Pecentage of Total Damage',
    fill='Damage Type',
    caption='Only the 20 most damaging event types are shown.'
  ) +
  theme_minimal() +
  theme(
    plot.caption = element_text(hjust = -0.75, face= "italic"),
    aspect.ratio=4/4
  )
```
This plot shows that not all highly damaging events caused the same types of damages. Flash floods and tornadoes predominantly caused damage to property, while droughts and freezes disproportionately affected crops.

# Event Severity Clustering Using K-means
## Preprocessing & Optimisation
### Feature Selection & Engineering
```{r Feature Selection & Engineering}
# Create combined metrics by combining direct and indirect
details <- details %>% 
  mutate(
    attributed_deaths = deaths_direct + deaths_indirect,
    attributed_injuries = injuries_direct + injuries_indirect,
    attributed_casualties = attributed_deaths + attributed_injuries
  )

# Select only relevant data and remove rows with NA
model_data <- details %>% 
  select(
    event_id, attributed_casualties, damage_total, event_type
  ) %>% 
    filter(
      !is.na(damage_total) & !is.na(attributed_casualties)
    )

# Z-Score normalise the data
model_data_scaled <- model_data %>%
  mutate(
    damage_total = as.vector(scale(damage_total)),
    attributed_casualties = as.vector(scale(attributed_casualties))
)
```

### K Hyperparameter Optimisation
```{r Cluster number optimisation, fig.align='center', eval=FALSE}
# Initialise empty lists
k_list = list()
sse_list = list()

# Loop through 1-30 clusters and store total sum of squared error
for (k in 1:30) {
  kmeans_spec <- k_means(num_clusters = k)
  
  kmeans_fit <- kmeans_spec %>%
    fit(~ damage_total + attributed_casualties, data = model_data_scaled)
  
  kmeans_result <- kmeans_fit$fit
  sse <- kmeans_result$tot.withinss
  
  k_list <- c(k_list, k)
  sse_list <- c(sse_list, sse)
}

# Convert lists to a DataFrame
elbow_data <- data.frame(unlist(k_list), unlist(sse_list))
names(elbow_data) = c("k","total_sse")


# Create elbow plot using clustering data
elbow_data %>% 
  ggplot(
    mapping=aes(
      x=k,
      y=total_sse
    )
  ) + 
  geom_line() +
  geom_point() + 
  scale_x_continuous(breaks = seq(1, 30, by = 1)) +
  labs(
    title='Total SSE by Number of Clusters', 
    x='Number of Clusters (k)',
    y='Total SSE'
  ) +
  theme_minimal()
```

## Fitting the Optimised Model
```{r Optimised Model}
# Create a model object using the optimum number of clusters
kmeans_spec <- k_means(num_clusters = 3)

# Fit the model
kmeans_fit <- kmeans_spec %>%
  fit(~ damage_total + attributed_casualties, data = model_data_scaled)

# Add clusters to data
model_data_scaled <- kmeans_fit %>% 
  augment(
    model_data_scaled
  )

# Change cluster names
model_data_scaled <- model_data_scaled %>% 
  rename(
    cluster = .pred_cluster
  ) %>% 
  mutate(
    cluster = case_when(
      cluster == 'Cluster_1' ~ 'Low Damage / Low Casualties',
      cluster == 'Cluster_2' ~ 'High Damage / Low Casualties',
      TRUE ~ 'Low Damage / High Casualties'
    )
  )
```

## Visualising Clustering Results
### Scatterplot using Clusters
```{r Scatter Plot of Scaled Data by Cluster, fig.align='center'}
# Create a scatterplot of the features, hued by cluster
model_data_scaled %>%
  ungroup() %>% 
    ggplot(
      mapping=aes(
        x=damage_total,
        y=attributed_casualties,
        colour=cluster
      )
    ) +
    geom_jitter() +
    labs(
      x='Scaled Total Damage',
      y='Scaled Attributed Casualties',
      title='K-Means Clustering of Casualties & Damages',
      colour='Cluster'
    ) +
    theme_minimal()
```

### Tabular Clusters
```{r Cluster Frequency Table, fig.align='center'}
# Create a table for the number of events in each cluster
model_table <- model_data_scaled %>% 
  group_by(
    cluster
  ) %>% 
    summarise(
      total = n()
    ) %>%
      rename(
        Cluster = cluster,
        Total = total
      ) %>% 
        arrange(
          desc(Total)
        )

kable(model_table)
```

Across the assessed time period, the vast majority of storm events fell into the low severity category with only a relative handful being found in the extremes of high damage or high casualties.

# Geospatial Correlation Between Events & Time
## Building the Function
```{r Map Correlations Between Number of Events & Time}
# Define function
map_correlations <- function(event_type) {
  # Defuse argument for dplyr
  event_type_expr <- enquo(event_type)
  
  # Group by region and year
  total_region_data <- details %>%
    select(
      begin_dt, event_type, region
    ) %>% 
      group_by(
        region, year(begin_dt)
      ) %>%
        summarise(
          ov_total = n()
        ) %>% 
          rename(
            year = `year(begin_dt)`
          )

  # Evaluate and filter by event type, join to total events
  region_data <- details %>%
    filter(
      event_type == !!event_type_expr
    ) %>% 
      select(
        begin_dt, event_type, region
      ) %>% 
        group_by(
          region, year(begin_dt)
        ) %>%
          summarise(
            total = n()
          ) %>% 
            arrange(
              desc(total)
            ) %>% 
              rename(
                year = `year(begin_dt)`
              ) %>% 
                inner_join(
                  total_region_data, by=c('region', 'year')
                ) %>% 
                  mutate(
                    prop = total / ov_total
                  )

  # Run pearson's rank between time & event proportion
  correlation_data <- region_data %>%
    group_by(
      region
    ) %>%
      summarise(
        # Continue through common, non-fatal errors
        corr_test = list(tryCatch(
          cor.test(
            year, 
            prop, 
            method = "pearson", 
            use = "complete.obs"
          ), 
          error = function(e) NULL
        ))
      ) %>%
        # Add correlation and p-values into variables
        mutate(
          corr = sapply(corr_test, function(test) if (!is.null(test)) test$estimate else NA),
          p_value = sapply(corr_test, function(test) if (!is.null(test)) test$p.value else NA)
        ) %>%
          # Remove NA values and statistically insignificant results
          filter(
            !is.na(corr) & !is.na(p_value) & p_value <= 0.05
          ) %>%
            select(
              region, corr, p_value
            ) %>%
              mutate(
                event_type = event_type
              )
  
  # Build a map of the US
  map_data <- usmap::us_map(regions = "states")
  
  # Mutate map_data to have parity with correlation data
  map_data <- map_data %>% 
    mutate(
      full = tolower(full)
    ) %>% 
      rename(
        region = full
      )
  
  # Alter correlation region data
  correlation_data <- correlation_data %>% 
    mutate(
      region = tolower(region)
    )
  
  # Join correlation and map data
  data <- left_join(map_data, correlation_data, by='region')
  
  # Build the map showing correlations
  plot_usmap(
    data=data, 
    values='corr'
  ) +
  scale_fill_continuous(name = "Correlation \nCoefficient") +
  theme(legend.position = "right") +
  labs(
    title=paste('Correlations Between Year &', event_type, 'Events as a Proportion of Total State Events'
          ),
    subtitle = 'Only statistically significant (p<=0.05) correlation coefficients are shown'
  )
}
```

## Using the Function {.panelset}

### Thunderstorm Wind

#### Thunderstorm Wind Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Thunderstorm Wind')
```

### Droughts

#### Drought Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Drought')
```

### Hurricanes

#### Hurricane Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Hurricane')
```

##
These plots highlight a key limitation with this script, rarer events are much harder to track and predict through time. As a result, lots of the potential correlations for rarer events, such as hurricanes or droughts, are not shown due to the statistical insignificance owing to a smaller sample size.

# Deadly Storm Predictor
## Preprocessing
### Response & Sampling
```{r}
# Create new deady variable based on deaths
details <- details %>% 
  mutate(
    deadly = case_when(
      deaths_direct > 0 | deaths_indirect > 0 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    )
  )

# Select only relevant variables
model_data <- details %>% 
  select(
    begin_dt, region, event_type, deadly
  )

# Take a large sample of the data
model_data <- model_data %>% 
  sample_n(
    1000000
  )
```

### Train, Test Split
```{r}
# Set seed and split data
set.seed(1)
storm_split <- initial_split(model_data)
storm_train <- training(storm_split)
storm_test  <- testing(storm_split)
```

### Building the Recipe, Model, & Workflow
```{r}
# Build preprocessing recipe
storm_rec_1 <- recipe(
  deadly ~ .,
  data = storm_train
) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# Declare model
storm_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")
  
# Build workflow using recipe and model
storm_wflow_1 <- workflow() %>%
  add_recipe(storm_rec_1) %>%
  add_model(storm_mod_1)
```

## Training the Model
### Fitting the Model Using Parallel Processing
```{r message=FALSE, warning=FALSE}
# Use parallel processing to speed up fit
registerDoParallel(cores = detectCores() - 1)

# Fit and store the model
storm_fit_1 <- fit(storm_wflow_1, data = storm_train)
```

### Extracting Prediction Probabilities
```{r message=FALSE, warning=FALSE}
# Create prediction probabilities and bind to test data
storm_pred <- predict(
  storm_fit_1, 
  storm_test, 
  type = "prob"
) %>%
  bind_cols(
    storm_test
  )
```

## Analysing Model Fit
### ROC Curve & AUC Value
```{r fig.align='center'}
# Convert response variable to a factor
storm_pred <- storm_pred %>% 
  mutate(
    deadly = as.factor(deadly)
  )

# Calculate an AUC score
auc_score <- storm_pred %>% 
  roc_auc(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  )

# Build ROC curve and attach AUC score
storm_pred %>%
  roc_curve(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  ) %>%
  autoplot() +
  labs(
    title = 'ROC Curve for Deadly Storm Predictor',
    x = 'False Positive Rate / 1 - Specificity',
    y = 'True Positive Rate / Sensitivity'
  ) +
  geom_text(
    x = 0.6,
    y = 0.45,
    label = paste("AUC = ", round(auc_score$.estimate, 2))
  )
```

### Building a Confusion Matrix
```{r message=FALSE, warning=FALSE, fig.align='center'}
# Use probability cutoff to calculate model prediction
storm_pred <- storm_pred %>% 
  mutate(
    prediction = case_when(
      .pred_Deadly >= 0.5 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    ),
    prediction = as.factor(prediction)
  )

# Construct and a confusion matrix object and convert it to a tibble
conf_mat <- conf_mat(
  data = storm_pred,
  truth = deadly,
  estimate = prediction
) %>%
  tidy()

# Extract values from the tibble to build a new, clean tibble
confusion <- tibble(
  truth = c('Deadly', 'Not Deadly', 'Deadly', 'Not Deadly'),
  prediction = c('Deadly', 'Not Deadly', 'Not Deadly', 'Deadly'),
  n = c(conf_mat[1,2], conf_mat[4,2], conf_mat[2,2], conf_mat[3,2])
)

# Plot a confusion matrix
confusion %>% 
  ggplot(
    mapping = aes(
      x = truth, 
      y = prediction
    )
  ) +
  geom_tile(fill = "steelblue",
            color = "black",
            size = 0.5
  ) +
  geom_text(
    mapping = aes(
      label = n
    ), 
    color = "black", 
    size = 5
  ) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix for Deadly Storm Predictor",
    x = "Model Prediction", 
    y = "Truth"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = 'none'
  )
```

### Accuracy
```{r}
# Calculate an accuracy metric
storm_pred %>% 
  mutate(
    result = case_when(
      deadly == prediction ~ 'Correct',
      TRUE ~ 'Incorrect'
    )
  ) %>% 
    group_by(
      result
    ) %>% 
      summarise(
        percentage = n()/250000*100
      ) %>%
        rename(
        Result = result,
        Percentage = percentage
      ) %>% 
        kable()
```