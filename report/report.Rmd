---
title: "Weather in the United States (1996-2023)"
author: "by Just Commit (Group number: 17): Olivia Harris, Maxwell Pohlman, Sarah Stewart, Helen Miller, Andrew Morris, & Adam Laycock"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = 'center')
```

```{r load-lib, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(janitor)
library(workflows)
library(parsnip)
library(tidyclust)
library(usmap)
library(gghighlight)
library(knitr)
library(xaringanExtra)
library(doParallel)
library(broom)
library(patchwork)

xaringanExtra::use_panelset()
```

# Research Question

How have storm events changed over time, and what are the personal and monetary effects of these changes?

We chose to investigate this question due to the increasingly dramatic shifts in climate observed in recent years.
Many of these changes seem to be significantly impacting the United States, and we wanted to explore those impacts in our data.

# Data

Our data was sourced from the Storm Events Database of the National Oceanic and Atmospheric Administration (NOAA).
Individual .csv files were downloaded and compiled using the script found in `data/compile_script`.
The NOAA collects data on weather events in the United States.
This includes information on event location, 50 variables related to event details (including, notably, time of occurrence and damages), and 10 variables related to event fatalities.
In our analysis, we focused on over 1.6 million observations collected between 1996 and 2023.

In our initial cleaning, we notably removed unwanted variables such as event narrative (a multi-sentence description of the event), merged date columns and converted them to date-times with `lubridate`, and added a column for total damage cost (summing crop and property damage).
All damage information was converted from a string to a double.
The cleaning script can be found in it's entirety below.

# Load Data

```{r load-data, eval=FALSE, echo=TRUE}
details = read_csv('data/original_data/details_combined.csv')
fatalities = read_csv('data/original_data/fatalities_combined.csv')
```

# Clean Data

```{r cleaning_script, eval=FALSE, echo=TRUE}
# Merge and convert formats of begin-date variables
details <- details %>%
  # Combine date elements
  unite("BEGIN_DATE", BEGIN_YEARMONTH, BEGIN_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    BEGIN_DATE = as.character(BEGIN_DATE),
    # Add leading zero if the time is 3 digits
    BEGIN_TIME = case_when(
      BEGIN_TIME < 1000 ~ sprintf("%04d", BEGIN_TIME),
      TRUE ~ as.character(BEGIN_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("BEGIN_DT", BEGIN_DATE, BEGIN_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    BEGIN_DT = ymd_hm(BEGIN_DT)
  )

# Merge and convert formats of end-date variables
details <- details %>%
  # Combine date elements
  unite("END_DATE", END_YEARMONTH, END_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    END_DATE = as.character(END_DATE),
    # Add leading zero if the time is 3 digits
    END_TIME = case_when(
      END_TIME < 1000 ~ sprintf("%04d", END_TIME),
      TRUE ~ as.character(END_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("END_DT", END_DATE, END_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    END_DT = ymd_hm(END_DT)
  )

# Remove unnecessary columns
details <- details %>%
  select(
    -c(YEAR, MONTH_NAME, BEGIN_DATE_TIME, END_DATE_TIME, 
       EVENT_NARRATIVE, EPISODE_NARRATIVE
    )
  )

# Rename incorrect column names
details <- details %>%
  rename(
    REGION = STATE,
    REGION_FIPS = STATE_FIPS
  )

# Create duration variable 
details <- details %>% 
  mutate(
    duration = END_DT - BEGIN_DT
  ) %>% 
    mutate(
      duration = as.numeric(duration) / 60
    )

# Change cost suffixes
details <- details %>% 
  mutate(
    DAMAGE_PROPERTY = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_PROPERTY))),
    DAMAGE_CROPS = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_CROPS)))
  )

# Create a total damage column using crops and property
details <- details %>%
  mutate(
    DAMAGE_TOTAL = DAMAGE_PROPERTY + DAMAGE_CROPS
  )

# Remove all records pertaining to an incomplete year
details <- details %>% 
  filter(
    !year(BEGIN_DT) == '2024'
  )

# Use janitor to convert variable names to snakecase
details <- details %>% 
  clean_names()

# Tidy the fatalities DataFrame
fatalities <- fatalities %>% 
  select(
    -FAT_TIME, -FATALITY_DATE, -EVENT_YEARMONTH
  ) %>% 
    mutate(
      YMD = ymd(paste(FAT_YEARMONTH, FAT_DAY))
    ) %>% 
      select(
        -FAT_DAY, -FAT_YEARMONTH
      )

# Remove 2024 records
fatalities <- fatalities %>%
  filter(
    !year(YMD) == '2024'
  )

# Use janitor to convert variables to snakecase
fatalities <- fatalities %>% 
  clean_names()

details <- details %>%
  mutate(
    event_type = recode(
      event_type,
      "Hurricane (Typhoon)" = "Hurricane"
    )
  )

#save cleaned data
#write_rds(details, "data/clean_data/details_clean.rds", compress = "gz")
#write_rds(fatalities, "data/clean_data/fatalities_clean.rds")
```

# Load Cleaned Data

```{r load_clean_data, echo=TRUE}
details = read_rds('data/clean_data/details_clean.rds')
fatalities = read_rds('data/clean_data/fatalities_clean.rds')
```

# Changes in Storm Event Frequency Over Time

To identify how storm events have changed over time we used a linear regression model attempting to predict the number of storm events per year.
We grouped event frequencies by year to balance statistical significance and trend visibility as smaller groups were skewed by outliers, while larger ones (e.g. 5-year periods) lacked sufficient detail to show trends.
To do this, we created the object `events_per_year` by summing event frequencies annually and ran the regression: `lm(event_count ~ year, data = events_per_year)`.
The results predicted for every one-year increase there will be, on average, an additional 774.2 storm events per year.
These results are statistically significant (p \< 0.01, R-squared = 0.536) with over half of the variation in `event_count` explained by the variable `year`.

We created similar models using fatalities and damages per year.
These models predict that for every additional year, on average, fatalities will increase by 15.3 individuals and damages will increase by \$447 million.
Both these results have p-values \< 0.05 and are statistically significant.
However, the R-squared values are low (0.3 for damages and 0.2 for fatalities), indicating there are other factors with a significant influence.

# Regional Exploration

```{r filter events}
severe_details <- details %>%
  select(
    -c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, 
       cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, 
       deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, 
       tor_length, tor_width, tor_other_wfo, tor_other_cz_state, 
       tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, 
       begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, 
       begin_range, begin_location, end_range)
  ) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) %>%
  distinct(event_type, .keep_all = TRUE) 
```

```{r not severe}
not_severe_details <- details %>%
  select(-c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, tor_length, tor_width, tor_other_wfo, tor_other_cz_state, tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, begin_range, begin_location, end_range)) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) 
```

```{r joining}
combined_data <- severe_details %>%
  full_join(fatalities, by = "event_id")
```

```{r not combined}
not_combined_data <- not_severe_details %>%
  full_join(fatalities, by = "event_id")
```

```{r no fatalities per region, warning=FALSE, message=FALSE}
fatalities_by_region <-not_combined_data%>%
  group_by(region) %>%
  summarize(
    no_fatalities = n_distinct(fatality_id)) %>%
  mutate(no_fatalities = no_fatalities) %>%
  ungroup()

total_fatalities <- not_combined_data %>%
  summarize(total_fatalities = n_distinct(fatality_id)) %>%
  pull(total_fatalities)
  
fatalities_by_region <- fatalities_by_region %>%
  mutate(total_fatalities= total_fatalities)

not_combined_data <- not_combined_data %>%
  left_join(fatalities_by_region, by="region")
```

In visualising the total number of weather events, we found that Texas experienced considerably more weather events than any other state.
This could potentially be explained by its considerable size.

```{r not severe frequency events, warning=FALSE, message=FALSE}
not_frequency_by_region <-not_combined_data%>%
  group_by(region) %>%
  summarise(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
not_total_events <- not_combined_data %>%
  summarise(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(not_total_events = 12233)
 
not_combined_data <- not_combined_data %>%
  left_join(not_frequency_by_region, by="region")
```

```{r not severe event frequency map, warning=FALSE, message=FALSE, fig.align='center'}
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(not_frequency_by_region, by = "region") 
 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of Events") + 
  labs(
    title = "Number of all Weather Events Experienced per State") +
  theme_minimal()
```

When comparing this data to the total number of fatalities caused by all weather events, we found that, as predicted, Florida, California, and Texas all experienced a high fatality rate.
However, Arizona showed an unexpectedly high number of fatalities, though it did not experience as many severe weather events.

```{r frequency events}
frequency_by_region <- combined_data %>%
  group_by(region) %>%
  summarise(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
total_events <- combined_data %>%
  summarise(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
frequency_by_region <- frequency_by_region %>%
  mutate(total_events = total_events)
 
combined_data <- combined_data %>%
  left_join(frequency_by_region, by="region")
```

```{r event frequency map, fig.align='center'}
combined_data <- combined_data %>% 
  mutate(region = tolower(region))
 
frequency_by_region <- frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(frequency_by_region, by = "region") 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of Severe Events") + 
  labs(
    title = "Map of the USA",
    subtitle = "Most severe weather events experienced per state") +
  theme_minimal()
```

```{r not fatalities map}
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
fatalities_by_region <- fatalities_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(fatalities_by_region, by = "region") 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_fatalities)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of Fatalities") + 
  labs(
    title = "Number of Fatalities per State") +
  theme_minimal()
```

Finally, we compared these results to the total damages caused by severe weather events.
Florida, Texas, California, and Arizona all experienced a high amount of total damage.
However, many states faced similarly high costs while experiencing drastically fewer severe events.
We concluded that these states potentially lack sufficient infrastructure to prepare for storm events, especially compared to states which experience these events frequently.

# Damage Makeup by Event Type

When exploring damages by event type, we found that not all highly damaging events caused the same type of damage.
Flash floods and tornadoes predominantly caused damage to property, while droughts and freezes disproportionately affected crops.

```{r fig.align='center'}
cost_data <- details %>% 
  select(
    event_type, damage_property, damage_crops
  ) %>% 
    group_by(
      event_type
    ) %>% 
      summarise(
        Property = sum(
          damage_property, 
          na.rm=TRUE
        ),
        Crops = sum(
          damage_crops, 
          na.rm=TRUE
        )
      ) %>%
        slice_max(
          order_by = Property + Crops, 
          n = 20
        ) %>% 
          pivot_longer(
            cols=c('Property', 'Crops'),
            names_to='type',
            values_to='damage'
          )

cost_data %>% 
  ggplot(
    mapping=aes(
      x=damage/sum(damage)*100,
      y=reorder(event_type, damage),
      fill=type
    )
  ) +
  geom_col() +
  labs(
    title = 'Damage by Storm Event Type between 1996 & 2023', 
    subtitle = 'Total Damage (Property + Crops) was $227bn',
    y = 'Type of Event',
    x = 'Pecentage of Total Damage',
    fill='Damage Type',
    caption='Only the 20 most damaging event types are shown.'
  ) +
  theme_minimal() +
  theme(
    plot.caption = element_text(hjust = -0.75, face= "italic"),
    aspect.ratio=4/4
  )
```

# Event Severity Clustering Using K-means

We were interested in determining if a k-means clustering algorithm could be used to group events.
Fatalities and damage were scaled and fed into a k-means model.
The optimum number of clusters was determined using an elbow plot and clusters were visualised.

```{r Feature Selection & Engineering}
# Create combined metrics by combining direct and indirect
details <- details %>% 
  mutate(
    attributed_deaths = deaths_direct + deaths_indirect,
    attributed_injuries = injuries_direct + injuries_indirect,
    attributed_casualties = attributed_deaths + attributed_injuries
  )

# Select only relevant data and remove rows with NA
model_data <- details %>% 
  select(
    event_id, attributed_casualties, damage_total, event_type
  ) %>% 
    filter(
      !is.na(damage_total) & !is.na(attributed_casualties)
    )

# Z-Score normalise the data
model_data_scaled <- model_data %>%
  mutate(
    damage_total = as.vector(scale(damage_total)),
    attributed_casualties = as.vector(scale(attributed_casualties))
)
```

## Elbow Plot

```{r Cluster number optimisation, fig.align='center'}
# Initialise empty lists
k_list = list()
sse_list = list()

# Loop through 1-30 clusters and store total sum of squared error
for (k in 1:30) {
  kmeans_spec <- k_means(num_clusters = k)
  
  kmeans_fit <- kmeans_spec %>%
    fit(~ damage_total + attributed_casualties, data = model_data_scaled)
  
  kmeans_result <- kmeans_fit$fit
  sse <- kmeans_result$tot.withinss
  
  k_list <- c(k_list, k)
  sse_list <- c(sse_list, sse)
}

# Convert lists to a DataFrame
elbow_data <- data.frame(unlist(k_list), unlist(sse_list))
names(elbow_data) = c("k","total_sse")


# Create elbow plot using clustering data
elbow_data %>% 
  ggplot(
    mapping=aes(
      x=k,
      y=total_sse
    )
  ) + 
  geom_line() +
  geom_point() + 
  scale_x_continuous(breaks = seq(1, 30, by = 1)) +
  labs(
    title='Total SSE by Number of Clusters', 
    x='Number of Clusters (k)',
    y='Total SSE'
  ) +
  theme_minimal()
```

```{r Optimised Model}
# Create a model object using the optimum number of clusters
kmeans_spec <- k_means(num_clusters = 3)

# Fit the model
kmeans_fit <- kmeans_spec %>%
  fit(~ damage_total + attributed_casualties, data = model_data_scaled)

# Add clusters to data
model_data_scaled <- kmeans_fit %>% 
  augment(
    model_data_scaled
  )

# Change cluster names
model_data_scaled <- model_data_scaled %>% 
  rename(
    cluster = .pred_cluster
  ) %>% 
  mutate(
    cluster = case_when(
      cluster == 'Cluster_1' ~ 'Low Damage / Low Casualties',
      cluster == 'Cluster_2' ~ 'High Damage / Low Casualties',
      TRUE ~ 'Low Damage / High Casualties'
    )
  )
```

## Scatterplot using Clusters

```{r Scatter Plot of Scaled Data by Cluster, fig.align='center'}
# Create a scatterplot of the features, hued by cluster
model_data_scaled %>%
  ungroup() %>% 
    ggplot(
      mapping=aes(
        x=damage_total,
        y=attributed_casualties,
        colour=cluster
      )
    ) +
    geom_jitter() +
    labs(
      x='Scaled Total Damage',
      y='Scaled Attributed Casualties',
      title='K-Means Clustering of Casualties & Damages',
      colour='Cluster'
    ) +
    theme_minimal()
```

## Tabular Clusters

```{r Cluster Frequency Table, fig.align='center'}
# Create a table for the number of events in each cluster
model_table <- model_data_scaled %>% 
  group_by(
    cluster
  ) %>% 
    summarise(
      total = n()
    ) %>%
      rename(
        Cluster = cluster,
        Total = total
      ) %>% 
        arrange(
          desc(Total)
        )

kable(model_table)
```

Across the assessed period, the vast majority of storm events fell into the low severity category with a minority in the high damage or high casualty clusters.

# Geospatial Correlation Between Events & Time

## Building the Function

The function below is used to visualise correlation coefficients between event proportions, for a particular event type, per year and time.
Only the statistically significant coefficients for each state are shown.

```{r Map Correlations Between Number of Events & Time, echo=TRUE}
# Define function
map_correlations <- function(event_type) {
  # Defuse argument for dplyr
  event_type_expr <- enquo(event_type)
  
  # Group by region and year
  total_region_data <- details %>%
    select(
      begin_dt, event_type, region
    ) %>% 
      group_by(
        region, year(begin_dt)
      ) %>%
        summarise(
          ov_total = n()
        ) %>% 
          rename(
            year = `year(begin_dt)`
          )

  # Evaluate and filter by event type, join to total events
  region_data <- details %>%
    filter(
      event_type == !!event_type_expr
    ) %>% 
      select(
        begin_dt, event_type, region
      ) %>% 
        group_by(
          region, year(begin_dt)
        ) %>%
          summarise(
            total = n()
          ) %>% 
            arrange(
              desc(total)
            ) %>% 
              rename(
                year = `year(begin_dt)`
              ) %>% 
                inner_join(
                  total_region_data, by=c('region', 'year')
                ) %>% 
                  mutate(
                    prop = total / ov_total
                  )

  # Run pearson's rank between time & event proportion
  correlation_data <- region_data %>%
    group_by(
      region
    ) %>%
      summarise(
        # Continue through common, non-fatal errors
        corr_test = list(tryCatch(
          cor.test(
            year, 
            prop, 
            method = "pearson", 
            use = "complete.obs"
          ), 
          error = function(e) NULL
        ))
      ) %>%
        # Add correlation and p-values into variables
        mutate(
          corr = sapply(corr_test, function(test) if (!is.null(test)) test$estimate else NA),
          p_value = sapply(corr_test, function(test) if (!is.null(test)) test$p.value else NA)
        ) %>%
          # Remove NA values and statistically insignificant results
          filter(
            !is.na(corr) & !is.na(p_value) & p_value <= 0.05
          ) %>%
            select(
              region, corr, p_value
            ) %>%
              mutate(
                event_type = event_type
              )
  
  # Build a map of the US
  map_data <- usmap::us_map(regions = "states")
  
  # Mutate map_data to have parity with correlation data
  map_data <- map_data %>% 
    mutate(
      full = tolower(full)
    ) %>% 
      rename(
        region = full
      )
  
  # Alter correlation region data
  correlation_data <- correlation_data %>% 
    mutate(
      region = tolower(region)
    )
  
  # Join correlation and map data
  data <- left_join(map_data, correlation_data, by='region')
  
  # Build the map showing correlations
  plot_usmap(
    data=data, 
    values='corr'
  ) +
  scale_fill_continuous(name = "Correlation \nCoefficient") +
  theme(legend.position = "right") +
  labs(
    title=paste('Correlations Between Year &', event_type, 'Events as a Proportion of Total State Events'
          ),
    subtitle = 'Only statistically significant (p<=0.05) correlation coefficients are shown'
  )
}
```

## Using the Function {.panelset}

### Thunderstorm Wind

#### Thunderstorm Wind Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Thunderstorm Wind')
```

### Droughts

#### Drought Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Drought')
```

### Hurricanes

#### Hurricane Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Hurricane')
```

## 

These plots highlight a key limitation with this script: rarer events are much harder to track and predict through time.
As a result, many potential correlations for these events, such as hurricanes or droughts, cannot be shown due to statistical insignificance owing to a smaller sample size.

# Deadly Storm Predictor

We also wanted to assess whether it was possible to predict if a storm would cause fatalities, using only information about the event that could be known beforehand.
We used a logistic regression model with the input variables of `begin_dt`, `region`, and `event_type`.

```{r}
# Create new deady variable based on deaths
details <- details %>% 
  mutate(
    deadly = case_when(
      deaths_direct > 0 | deaths_indirect > 0 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    )
  )

# Select only relevant variables
model_data <- details %>% 
  select(
    begin_dt, region, event_type, deadly
  )

# Take a large sample of the data
model_data <- model_data %>% 
  sample_n(
    1000000
  )
```

```{r}
# Set seed and split data
set.seed(1)
storm_split <- initial_split(model_data)
storm_train <- training(storm_split)
storm_test  <- testing(storm_split)
```

```{r}
# Build preprocessing recipe
storm_rec_1 <- recipe(
  deadly ~ .,
  data = storm_train
) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# Declare model
storm_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")
  
# Build workflow using recipe and model
storm_wflow_1 <- workflow() %>%
  add_recipe(storm_rec_1) %>%
  add_model(storm_mod_1)
```

```{r message=FALSE, warning=FALSE}
# Use parallel processing to speed up fit
registerDoParallel(cores = detectCores() - 1)

# Fit and store the model
storm_fit_1 <- fit(storm_wflow_1, data = storm_train)
```

```{r message=FALSE, warning=FALSE}
# Create prediction probabilities and bind to test data
storm_pred <- predict(
  storm_fit_1, 
  storm_test, 
  type = "prob"
) %>%
  bind_cols(
    storm_test
  )
```

## Assessing Model Fit

```{r fig.align='center'}
# Convert response variable to a factor
storm_pred <- storm_pred %>% 
  mutate(
    deadly = as.factor(deadly)
  )

# Calculate an AUC score
auc_score <- storm_pred %>% 
  roc_auc(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  )

# Build ROC curve and attach AUC score
storm_pred %>%
  roc_curve(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  ) %>%
  autoplot() +
  labs(
    title = 'ROC Curve for Deadly Storm Predictor',
    x = 'False Positive Rate / 1 - Specificity',
    y = 'True Positive Rate / Sensitivity'
  ) +
  geom_text(
    x = 0.6,
    y = 0.45,
    label = paste("AUC = ", round(auc_score$.estimate, 2))
  )
```

```{r message=FALSE, warning=FALSE, fig.align='center'}
# Use probability cutoff to calculate model prediction
storm_pred <- storm_pred %>% 
  mutate(
    prediction = case_when(
      .pred_Deadly >= 0.5 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    ),
    prediction = as.factor(prediction)
  )

# Construct and a confusion matrix object and convert it to a tibble
conf_mat <- conf_mat(
  data = storm_pred,
  truth = deadly,
  estimate = prediction
) %>%
  tidy()

# Extract values from the tibble to build a new, clean tibble
confusion <- tibble(
  truth = c('Deadly', 'Not Deadly', 'Deadly', 'Not Deadly'),
  prediction = c('Deadly', 'Not Deadly', 'Not Deadly', 'Deadly'),
  n = c(conf_mat[1,2], conf_mat[4,2], conf_mat[2,2], conf_mat[3,2])
)

# Plot a confusion matrix
confusion %>% 
  ggplot(
    mapping = aes(
      x = truth, 
      y = prediction
    )
  ) +
  geom_tile(fill = "steelblue",
            color = "black",
            size = 0.5
  ) +
  geom_text(
    mapping = aes(
      label = n
    ), 
    color = "black", 
    size = 5
  ) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix for the Deadly Storm Predictor",
    x = "Model Prediction", 
    y = "Truth"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = 'none'
  )
```

```{r}
# Calculate an accuracy metric
storm_pred %>% 
  mutate(
    result = case_when(
      deadly == prediction ~ 'Correct',
      TRUE ~ 'Incorrect'
    )
  ) %>% 
    group_by(
      result
    ) %>% 
      summarise(
        percentage = n()/250000*100
      ) %>%
        rename(
        Result = result,
        Percentage = percentage
      ) %>% 
        kable()
```

Overall, the model metrics appeared to indicate that the fit was appropriate and was able to generalise when encountering new events.

# Event Type Averages

We split event types into two categories: more damaging/deadly and less damaging/deadly (relative to the mean).
This was used to assess whether increased damages and fatalities over time were reflected equally in both severe and less severe storm events.

```{r}
# Rename redundant event type names
details <- details %>%
  mutate(event_type = replace(event_type, event_type == "Hurricane (Typhoon)", "Hurricane"))
 
# Select relevant columns and replace NA values
details_selected <- details %>%
  select(event_type, deaths_direct, deaths_indirect, damage_total, begin_dt) %>%  
  mutate(
    year = as.numeric(format(as.Date(begin_dt, format = "%Y-%m-%d"), "%Y"))  
  ) %>%
  na.omit()
 
# Calculate total fatalities and damage per event, and group by event type
details_grouped <- details_selected %>%
  mutate(
    fatalities_total = deaths_direct + deaths_indirect
  ) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),  
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)  
  ) %>%
  ungroup()
 
# Calculate global averages and assign groups
global_averages <- details_grouped %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),  
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)  
  )
 
# Add grouping for more/less deadly and damaging based on global averages
details_grouped <- details_grouped %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
```

We used mean impact for the analysis of changes in fatalities and damages due to the possibility of stronger storms overall or greater storm frequency over time.

```{r fig.align='center'}
# Adjusted Graph for Fatalities (All Data)
fatality_graph <- details_grouped %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("More Deadly" = "red", "Less Deadly" = "green")) +
  labs(
    title = "Average Fatalities by Group",
    subtitle = "Based on Event Type Averages Compared to Global Average",
    x = "Weather Group",  
    y = "Average Fatalities per Event Type",
    fill = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.subtitle = element_text(size = 6.5),  
    axis.text.x = element_text(size = 5)  
  )
 
# Adjusted Graph for Damages (All Data)
damage_graph <- details_grouped %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("More Damaging" = "purple", "Less Damaging" = "orange")) +
  labs(
    title = "Average Damages by Group",
    subtitle = "Based on Event Type Averages Compared to Global Average",
    x = "Weather Group", 
    y = "Average Damages per Event Type (USD)",
    fill = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.subtitle = element_text(size = 6.5), 
    axis.text.x = element_text(size = 5)  
  )
 
# Combine  plots
combined_original <- fatality_graph + damage_graph
 
# Display plots
combined_original
```

This process was repeated for 1996–7 and 2022–3, using two-year periods to minimise the impact of outliers.

```{r}
# Filter for 1996
details_1996_7 <- details_selected %>% filter(year %in% c(1996, 1997))
details_grouped_1996_7 <- details_1996_7 %>%
  mutate(fatalities_total = deaths_direct + deaths_indirect) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)
  ) %>%
  ungroup()
 
# Recalculate global averages for 1996 data
global_averages_1996_7 <- details_grouped_1996_7 %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)
  )
 
# Add grouping for 1996 data
details_grouped_1996_7 <- details_grouped_1996_7 %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages_1996_7$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages_1996_7$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
```

```{r}
# Adjusted Graph for 1996 Fatalities
fatality_graph_1996_7 <- details_grouped_1996_7 %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "1996-7: Fatalities by Group", subtitle = "Data from 1996-7", 
       x = "Weather Group",  
       y = "Average Fatalities") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5) 
  )
 
# Adjusted Graph for 1996 Damages
damage_graph_1996_7 <- details_grouped_1996_7 %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "1996-7: Damages by Group", subtitle = "Data from 1996-7", 
       x = "Weather Group",  
       y = "Average Damages (USD)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Combine plots
combined_1996_7 <- fatality_graph_1996_7 + damage_graph_1996_7
 
# Display plots
combined_1996_7
```

```{r}
# Filter for 2022-3
details_2022_3 <- details_selected %>% filter(year %in% c(2022, 2023))
details_grouped_2022_3 <- details_2022_3 %>%
  mutate(fatalities_total = deaths_direct + deaths_indirect) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)
  ) %>%
  ungroup()
 
# Recalculate global averages for 2022-3 data
global_averages_2022_3 <- details_grouped_2022_3 %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)
  )
 
# Add grouping for 2022-3 data
details_grouped_2022_3 <- details_grouped_2022_3 %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages_2022_3$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages_2022_3$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
 
# Adjusted Graph for 2022-3 Fatalities
fatality_graph_2022_3 <- details_grouped_2022_3 %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "2022-3: Fatalities by Group", subtitle = "Data from 2022-3", 
       x = "Weather Group",  
       y = "Average Fatalities") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Adjusted Graph for 2022-3 Damages
damage_graph_2022_3 <- details_grouped_2022_3 %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "2022-3: Damages by Group", subtitle = "Data from 2022-3", 
       x = "Weather Group",  
       y = "Average Damages (USD)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Combine plots
combined_2022_3 <- fatality_graph_2022_3 + damage_graph_2022_3
 
# Display plots
combined_2022_3
```

We found that the averages of damages and fatalities decreased from 1996–7 to 2022–3.
By creating and joining the year groups, we were able to produce a table showing the percent and absolute change in the overall average of each period.
Finally, percent change was calculated for the more damaging/deadly and less damaging/deadly categories.

```{r}
# Create year groups
details_grouped_1996_7 <- details_grouped_1996_7 %>%
  mutate(year_group = "1996-1997")
 
details_grouped_2022_3 <- details_grouped_2022_3 %>%
  mutate(year_group = "2022-2023")
 
# Combine the data-sets
combined_data <- full_join(
  details_grouped_1996_7, 
  details_grouped_2022_3, 
  by = "event_type", 
  suffix = c("_1996_7", "_2022_3")
)
 
# Calculate differences and percentage changes
combined_data <- combined_data %>%
  mutate(
    change_fatalities = avg_fatalities_by_type_2022_3 - avg_fatalities_by_type_1996_7,
    change_damage = avg_damage_by_type_2022_3 - avg_damage_by_type_1996_7,
    percent_change_fatalities = (change_fatalities / avg_fatalities_by_type_1996_7) * 100,
    percent_change_damage = (change_damage / avg_damage_by_type_1996_7) * 100
  )
 
# Compare the global averages between the two groups
comparison <- tibble(
  metric = c("Fatalities", "Damages"),
  avg_1996_7 = c(global_averages_1996_7$global_avg_fatalities, global_averages_1996_7$global_avg_damage),
  avg_2022_3 = c(global_averages_2022_3$global_avg_fatalities, global_averages_2022_3$global_avg_damage),
  absolute_change = avg_2022_3 - avg_1996_7,
  percent_change = (absolute_change / avg_1996_7) * 100
)
 
# View
kable(comparison)
 
# Final category changes calculations
Percent_change_less_fatalities <- ((.02-.03)/.03)*100
Percent_change_more_fatalities <- ((.55-1.07)/1.07)*100
Percent_change_less_damages <- ((46507.11-384199.67)/384199.67)*100
Percent_change_more_damages <- ((6543117.28-7033105.21)/7033105.21)*100 
```

```{r, echo=TRUE}
# Laying it all out
Percent_change_less_fatalities
Percent_change_more_fatalities
Percent_change_less_damages
Percent_change_more_damages
```

While total damage and fatalities increased, the average impact per event declined.
This suggests that increased storm frequency has had a larger impact on people and property than storm strength.
However, we should note that this dataset had no variable representing infrastructure strength, which could confound our results.

# Event Proportions and ENSO Status

We were also interested in if the impact of El Niño Southern Oscillation (ENSO) was visible in our data.
To simplify this investigation, irrelevant variables were removed from the `details_clean` data set.
Then, using data from NOAA and NASA, a column displaying the ENSO status of each year was added and the resulting data frame was saved as `details_enso`.
Finally, a table of expected and observed events grouped by ENSO status was produced for initial analysis.
`exp_prop` acts as a null hypothesis-- assuming ENSO status has no impact on event frequency.

```{r}
#remove irrelevant variables for ENSO analysis
details_smaller <- details %>% 
  select(-region_fips,
         -cz_type, 
         -cz_fips, 
         -cz_timezone, 
         -wfo, 
         -source, 
         -tor_other_cz_state, 
         -tor_other_cz_fips, 
        )
```

```{r}
#add a column for ENSO status
details_enso <- details_smaller %>% 
  mutate(enso_status = case_when(
         year(begin_dt) %in% c(1998, 
                               2003, 
                               2007, 
                               2010, 
                               2016, # 1998-2022 -> (NOAA, 2022)
                               2023 #2023 > (NASA, 2024)
                               ) ~ 'el_nino',
         year(begin_dt) %in% c(1999, 
                               2000, 
                               2008, 
                               2011, 
                               2012, 
                               2021, 
                               2022
                               ) ~ 'la_nina',
         TRUE ~ 'neutral'
  ))
```

```{r}
#create frequency table of ENSO events
details_enso %>% 
  group_by(enso_status) %>% 
  summarise(obs_count = n(), #observed event count
            obs_prop = n() / nrow(details_enso), #observed event proportion
            exp_prop = length(unique(year(begin_dt))) #expected event proportion
            / length(unique(year(details_enso$begin_dt)))
  ) %>% 
    kable()
```

The resulting table shows that the observed frequency is relatively similar to expected, meaning we fail to reject the null hypothesis.

## Event Types and ENSO Status

Though event frequency does not appear significantly impacted by ENSO, we were also interested in if the proportion of event types varies by ENSO status.
This was explored by calculating event type proportions across all years of a common ENSO status.
Event types with the largest difference in proportion between El Niño and La Niña years were visualised.

```{r}
#calculate proportion in El Niño years
el_nino_prop <- details_enso %>% 
  filter(enso_status == 'el_nino') %>%
  count(event_type) %>% 
  mutate(proportion_en = n / sum(n))

#calculate proportion in La Niña years
la_nina_prop <- details_enso %>% 
  filter(enso_status == 'la_nina') %>%
  count(event_type) %>% 
  mutate(proportion_ln = n / sum(n))

#calculate proportion in neutral years
neutral_prop <- details_enso %>% 
  filter(enso_status == 'neutral') %>%
  count(event_type) %>% 
  mutate(proportion_n = n / sum(n))

#join proportion tibbles
proportions_enso <- full_join(el_nino_prop, neutral_prop, by = 'event_type') %>% 
  full_join(la_nina_prop, by = 'event_type') %>% 
  top_n((abs(proportion_en - proportion_ln)), n = 10) %>% 
  pivot_longer(cols = c(proportion_en, proportion_ln, proportion_n), 
               names_to = 'enso_status', 
               values_to = 'proportion'
               )
```

```{r chunk_5}
#reorder ENSO status for plotting
proportions_enso <- proportions_enso %>%
  mutate(enso_status = factor(enso_status, levels = c("proportion_en", "proportion_n", "proportion_ln")))

#create visualisation
proportions_enso %>% 
  ggplot(aes(fill = enso_status, 
             x = proportion, 
             y = reorder(event_type, proportion)) #make plot more intuitive
         ) +
  
  geom_col(position = 'dodge') + #separate columns -> easier to compare
  
  labs(
    x = "Proportion of Total Events in Common ENSO Status Years",
    y = "Event Type",
    title = "Event Proportion by Event Type and ENSO Status",
    subtitle = "Event types with largest difference between El Niño and La Niña years",
    fill = 'ENSO Status'
  ) +
  
  scale_fill_manual(values = c('proportion_en' = 'midnightblue', 
                               'proportion_n' = 'purple2', 
                               'proportion_ln' = 'mediumorchid2'),
    labels = c('El Niño', 'Neutral', 'La Niña')
  )
```

The visualisation above does allow us to see the impact of ENSO on weather in the United States.
For example, events such as drought seem to be much more common in La Niña years, as expected in the Eastern Pacific.
Additionally, the extreme weather that characterises El Niño years can be seen in the higher proportions of excessive heat and frost/freeze in comparison to other ENSO statuses.
Interestingly, floods and flash floods are more common in non-neutral ENSO years, possibly indicating a less significant impact on these storm events.

## Change in ENSO Statuses Over Time

As a part of initial analysis, we noted a change in the slopes of event count grouped by ENSO status, as seen in the figures below.
Events in La Niña years appear to be increasing in monthly frequency at a slightly greater rate than those of a different ENSO status.

```{r}
slope_exploration <- details_enso %>%
  #group by month to increase number of observations, avoid daily outliers
  group_by(floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))
  
#create visualisation (including individual observations)
slope_exploration %>% 
  ggplot(mapping = aes(x = floor_date, y = log(n), color = enso_status)) +
  
  geom_point() +
  
  geom_smooth(method = lm, se = FALSE) + #add linear model
  
  scale_color_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
    labels = c('El Niño', 'Neutral', 'La Niña')
  ) +
  
  labs(
    x = "Date",
    y = "Number of Events per Month (log-scale)",
    title = "Event Count Over Time, Grouped by ENSO Status",
    subtitle = "Includes Monthly Observations and Linear Model",
    color = "ENSO Status"
  )
```

```{r chunk_7}
#plot only linear models, excluding individual observations
slope_exploration %>% 
  ggplot(mapping = aes(x=floor_date, y = log(n), color = enso_status)) +
  
  geom_smooth(method = lm, se= FALSE) +
  
  scale_color_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
    labels = c('El Niño', 'Neutral', 'La Niña')) +
  
  labs(
    x = "Date",
    y = "Number of Events per Month (log scale)",
    title = "Event Count Over Time, Grouped by ENSO Status",
    subtitle = "Includes Only Linear Model",
    color = "ENSO Status"
  )
  
```

To test if this difference in slope is statistically significant, each linear model was bootstrapped using 1000 samples.
The full procedure is below.
The results of the bootstrapping procedure including confidence intervals were then plotted.
No significant difference is apparent due to the overlap in confidence intervals, though this could be due to sample size.

```{r echo=TRUE}
#calculate number of events per month for each ENSO status
#store in seperate tibbles
by_month_en <- details_enso %>% 
  filter(enso_status == 'el_nino') %>% 
  group_by(en_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

by_month_ln <- details_enso %>%
  filter(enso_status == 'la_nina') %>% 
  group_by(ln_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

by_month_n <- details_enso %>%
  filter(enso_status == 'neutral') %>% 
  group_by(n_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

#create bootstrapping function
bootstrapping <- function(df, floor_name){
  # set a seed
  set.seed(30305)
  floor_name <- sym(floor_name)
  
  # take 1000 bootstrap samples
  enso_boot <- bootstraps(df, times = 1000)
  
  # for each sample
  # fit a model and save output in model column
  # tidy model output and save in coef_info column 
  enso_slope_models <- enso_boot %>%
    mutate(
      model = map(splits, ~ lm(n ~ !!floor_name, data = .)),
      coef_info = map(model, tidy)
    )
    
  # un-nest coef_info (for intercept and slope)
  enso_coefs <- enso_slope_models %>%
    unnest(coef_info)
    
  # calculate 95% confidence interval
  int_pctl(enso_slope_models, coef_info)
}

#display bootstrapped slopes in a tibble
el_nino_slope <- bootstrapping(by_month_en, 'en_floor_date')
el_nino_slope %>% kable()
la_nina_slope <- bootstrapping(by_month_ln, 'ln_floor_date')
la_nina_slope %>% kable()
neutral_slope <- bootstrapping(by_month_n, 'n_floor_date')
neutral_slope %>% kable()
```

```{r}
#assign ENSO status to bootstrapped results before binding
el_nino_slope$enso_status <- "el_nino"
la_nina_slope$enso_status <- "la_nina"
neutral_slope$enso_status <- "neutral"

combined_slopes <- dplyr::bind_rows(el_nino_slope, neutral_slope, la_nina_slope)

#remove intercept error bars from bound tibble
combined_slopes %>% 
  filter(term != '(Intercept)') %>% 
  
  #create visualisation 
  ggplot(mapping = aes(x = enso_status, y = .estimate, fill = enso_status)) +
  
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) + 
  
  #assign CI values to error bar
  geom_errorbar(aes(ymin = .lower, ymax = .upper), 
                position = position_dodge(width = 0.8),
                width = 0.25
                ) +
  
  scale_fill_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
  labels = c('El Niño', 'La Niña', 'Neutral')
  ) +
  
  #correct axis labelling
  scale_x_discrete(
    labels = c('el_nino' = 'El Niño', 
               'la_nina' = 'La Niña', 
               'neutral' = 'Neutral'
               )
    ) +
  
  labs(
    x = "ENSO Status",
    y = "True Slope (of log-scaled model)",
    title = "Bootstrapped Linear Model Slopes by ENSO Status",
  ) +
  
  theme(legend.position = 'none')
```

# Fatalities and Storm Events

```{r Who is most affected by weather}
fatalities %>%
  ggplot(mapping = aes(x = fatality_age)) +
    geom_histogram(binwidth = 4) +
  labs(x="Ages", y ="Number of fatalities", title="Frequency Plot of Fatalities With Respect to Age") +
  theme_bw()
```

Regarding the `fatalities` dataset, we were interested in groups that appeared to be underrepresented.
We chose those aged 0-13 and 70+ as the groups of interest.
To simplify the analysis, we separated our weather events into more general categories.

```{r seperate weather events}
events <- details %>% select(event_id, event_type)

# separating events into categories
# Marine - water related events or events relating to bodies of water
# snow/ice - events related to cold or winter weather
# Atmospheric - electric storms and other atmospheric events
# Tropical - tropical weather events, hurricanes etc
# heat/temp - events relating towards high temperatures or fire
# rain - rain and hail
events <-events %>% 
  mutate(weather_cat = case_when(
    event_type == "Astronomical Low Tide" ~ "Marine",
    event_type == "Coastal Flood" ~ "Marine",
    event_type == "Debris Flow" ~ "Marine",
    event_type == "Flash Flood" ~ "Marine",
    event_type == "Flood" ~ "Marine",
    event_type == "High Surf" ~ "Marine",
    event_type == "Lake-Effect Snow" ~ "Marine",
    event_type == "Lakeshore Flood" ~ "Marine",
    event_type == "Marine Hail" ~ "Marine",
    event_type == "Marine High Wind" ~ "Marine",
    event_type == "Marine Strong Wind" ~ "Marine",
    event_type == "Marine Thunderstorm Wind" ~ "Marine",
    event_type == "Rip Current" ~ "Marine",
    event_type == "Seiche" ~ "Marine",
    event_type == "Storm Surge/Tide" ~ "Marine",
    event_type == "Tsunami" ~ "Marine",
    event_type == "Waterspout" ~ "Marine",
    event_type == "Sneakerwave" ~ "Marine",
    event_type == "Marine Lightning" ~ "Marine",
    event_type == "Marine Tropical Depression" ~ "Marine",
    event_type == "Marine Hurricane/Typhoon" ~ "Marine",
    event_type == "Marine Dense Fog" ~ "Marine",
    event_type == "Avalanche" ~ "Ice/Snow",
    event_type == "Blizzard" ~ "Ice/Snow",
    event_type == "Cold/Wind Chill" ~ "Ice/Snow",
    event_type == "Extreme Cold/Wind Chill" ~ "Ice/Snow",
    event_type == "Frost/Freeze" ~ "Ice/Snow",
    event_type == "Ice Storm" ~ "Ice/Snow",
    event_type == "Heavy Snow" ~ "Ice/Snow",
    event_type == "Sleet" ~ "Ice/Snow",
    event_type == "Winter Storm" ~ "Ice/Snow",
    event_type == "Winter Weather" ~ "Ice/Snow",
    event_type == "Dense Fog" ~ "Atmospheric",
    event_type == "Dense Smoke" ~ "Atmospheric",
    event_type == "Freezing Fog" ~ "Atmospheric",
    event_type == "Funnel Cloud" ~ "Atmospheric",
    event_type == "High Wind" ~ "Atmospheric",
    event_type == "Lightning" ~ "Atmospheric",
    event_type == "Strong Wind" ~ "Atmospheric",
    event_type == "Northern Lights" ~ "Atmospheric",
    event_type == "Thunderstorm Wind" ~ "Atmospheric",
    event_type == "Tornado" ~ "Atmospheric",
    event_type == "Dust Devil" ~ "Atmospheric",
    event_type == "Dust Storm" ~ "Atmospheric",
    event_type == "Hurricane (Typhoon)" ~ "Tropical",
    event_type == "Hurricane" ~ "Tropical",
    event_type == "Tropical Depression" ~ "Tropical",
    event_type == "Tropical Storm" ~ "Tropical",
    event_type == "Heat" ~ "Fire/HiTemps",
    event_type == "Excessive Heat" ~ "Fire/HiTemps",
    event_type == "Volcanic Ash" ~ "Fire/HiTemps",
    event_type == "Volcanic Ashfall" ~ "Fire/HiTemps",
    event_type == "Drought" ~ "Fire/HiTemps",
    event_type == "Wildfire" ~ "Fire/HiTemps",
    TRUE ~ "Rain"
  ))


```

```{r median age group}
median_fatality <- filter(fatalities, fatality_age < 70 & fatality_age > 13)
median_fatality <- right_join(events, median_fatality)
```

```{r old age group}
elderly_fatality <- filter(fatalities, fatality_age >= 70)
elderly_fatality <- right_join(events, elderly_fatality)
```

```{r young age group}
young_fatality <- filter(fatalities, fatality_age <= 13) 
young_fatality <- right_join(events, young_fatality)
```

```{r}
# Combining fatalities
median_fatality <- median_fatality %>%
  add_column(age_group = "median")

elderly_fatality <- elderly_fatality %>%
  add_column(age_group = "elderly")

young_fatality <- young_fatality %>%
  add_column(age_group = "young")

old_young <- rbind(elderly_fatality, young_fatality)
combine_fatality <- rbind(old_young, median_fatality)

```

```{r}
# plot of age groups and fatality type
ggplot(na.omit(combine_fatality), aes(x=age_group, fill=weather_cat)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2") +
  labs(x= "Age group", y= "Proportion of fatalities for each category", fill= "Weather Categories", title = "Proportion of Each Age Group Affected by Each Weather Category") +
  theme_bw() # Proportion of each age group affected by different weather categories labelled above
```

This allows us to see how different age groups are disproportionately affected by different types of weather, giving us a better understanding of who is most vulnerable to storm events.
For example, we can see that both `Fire/HiTemps` and `Tropical` events are most dangerous for the elderly group, while `Atmospheric` and `Marine` events are most dangerous for younger individuals.

```{r}
elderly_category <- filter(elderly_fatality, weather_cat == "Fire/HiTemps" | weather_cat == "Tropical") %>%
ggplot(aes(x=event_type, fill=weather_cat)) +
  geom_bar() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x ="Weather Event", y ="Amount of Fatalities", title="Elderly Fatalities by Most Popular Weather Categories", fill="Weather Category") +
  theme_bw()+
  coord_flip() 

elderly_category
```

```{r}
young_category <- filter(young_fatality, weather_cat == "Marine" | weather_cat == "Atmospheric") %>%
ggplot(aes(x=event_type, fill=weather_cat)) +
  geom_bar() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x ="Weather Event", y ="Amount of Fatalities", title="Young Fatalities by Most Popular Weather Categories", fill="Weather Category") +
  theme_bw()+
  coord_flip() # young fatality age group plotted with the 2 largest disproportionate causes of fatalities

young_category
```

Specifically, we found that the younger age group is most affected by floods and tornadoes, whilst the elderly group is most affected by heat and hurricanes.

```{r}
combine_facet <- ggplot(na.omit(combine_fatality), aes(x=age_group, fill=weather_cat)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2") +
  labs(x= "Age group", y= "Proportion of fatalities for each category over time", fill= "Weather Categories", title = "Proportion of Each Age Group Affected by Each Weather Category") +
  scale_x_discrete(labels = c(
    "elderly" = "E",
    "median" = "M",
    "young" = "Y"
  )) +
  theme_bw() +
  theme(axis.text.y = element_blank()) +
  facet_wrap(year(ymd)~.) 

combine_facet
```

We can also see how these proportions have changed over the years of recorded data, and even note that in years of extraordinary weather (such as Hurricane Katrina in 2005 and a set of major tornadoes in 2011), weather events will disproportionately affect different age groups.

```{r}
# are Fire/HiTemps increasing?

combine_lines <- filter(combine_fatality, weather_cat == "Fire/HiTemps") %>%
  ggplot(aes(x=year(ymd), colour=event_type)) +
  geom_line(aes(fill=..count..),stat="bin", binwidth = 1, linewidth = 1) +
  labs(x="Years",y="Amount of Fatalities",title="Amount of Fire and High Temperature Fatalities Over Time",colour="Event Types") +
  scale_colour_manual(values=c("midnightblue", "purple2", "mediumorchid2")) +
  theme_bw() 

# plot looks at the changes of Fire/HiTemps weather_cat over years

combine_lines

combine_hist <- filter(na.omit(combine_fatality), event_type == "Heat" & year(ymd) >= "1995" & year(ymd) <= "2008") %>%
  ggplot(aes(x=year(ymd))) +
  geom_histogram(binwidth = 1) +
  labs(y="Amount of fatalities",x="Years",title="Unusually High Fire and High Temperature Fatalities Between 1998 and 2003") +
  theme_bw() # Histogram plot of extraordinary data between 1998 and 2003

combine_hist
```

We also found that, over the past 20 years, the fatalities resulting from `Fire/HiTemps` has increased.
This could possibly be the result of climate change impacting weather in the United States.

## References

Anderson, E.C.
(n.d.).
*Making Maps With R · Reproducible Research.* [online] eriqande.github.io.
Available at: <https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html> [Accessed Nov. 2024].

NASA Jet Propulsion Lab-- California Institute of Technology (2024).
*El Niño 2023 \| El Niño/La Niña Watch & PDO*.
[online] Ocean Surface Topography from Space.
Available at: <https://sealevel.jpl.nasa.gov/data/el-nino-la-nina-watch-and-pdo/el-nino-2023/> [Accessed Nov. 2024].

National Oceanic and Atmospheric Administration (2023).
*Past Events*.
[online] Noaa.gov.
Available at: <https://psl.noaa.gov/enso/past_events.html> [Accessed Nov. 2024].

NOAA Weather service, provided data for the project, accessed at URL: <https://www.ncdc.noaa.gov/stormevents/faq.jsp> NOAA Weather service, data format guide, accessed at URL: <https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf>
